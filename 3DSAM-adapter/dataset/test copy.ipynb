{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_encoder import PromptEncoder, TwoWayTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n",
      "[ 3683 13938 10786 16275 13296  3628 14136  3512  3113  9252]\n",
      "tensor([[ 99],\n",
      "        [114],\n",
      "        [ 34],\n",
      "        [ 19],\n",
      "        [112],\n",
      "        [ 44],\n",
      "        [ 56],\n",
      "        [ 56],\n",
      "        [ 41],\n",
      "        [ 36]])\n",
      "torch.Size([1, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seg = torch.ones(1,128,128)\n",
    "l = len(torch.where(seg == 1)[0])\n",
    "print(l)\n",
    "sample = np.random.choice(np.arange(l), 10, replace=True) # 從範圍為 [0, l) 的整數中隨機選取 10 個數字（可能有重複）\n",
    "print(sample)\n",
    "x = torch.where(seg == 1)[1][sample].unsqueeze(1)\n",
    "# y = torch.where(seg == 1)[3][sample].unsqueeze(1)\n",
    "z = torch.where(seg == 1)[2][sample].unsqueeze(1)\n",
    "print(z)\n",
    "point_coord = torch.cat([x, z], dim=1).unsqueeze(1).float() \n",
    "\n",
    "foo = torch.randn(1,20,2)\n",
    "point_coord = point_coord.transpose(0,1)\n",
    "point_coord = torch.cat([point_coord,foo],dim=1)\n",
    "print(point_coord.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "送進transformer的三個參數image_embeddings, image_pe, point_coord torch.Size([1, 1, 256, 64, 64]) torch.Size([1, 256, 32, 32, 32]) torch.Size([1, 1, 1, 30, 3])\n",
      "===init===\n",
      "image_embedding init torch.Size([1, 1, 256, 64, 64])\n",
      "point_coord init torch.Size([1, 1, 1, 30, 3])\n",
      "\n",
      "point_embedding after grid sample torch.Size([1, 1, 1, 1, 30])\n",
      "point_pe after grid sample torch.Size([1, 256, 1, 1, 30])\n",
      "\n",
      "        之所以維度由[1,256,32,32,32]變成[1,256,1,1,30], 是因為point_coord [1,1,1,30,3]中包含了30個xyz的座標(已正規化到-1~1之間)\n",
      "        定位了在image_embedding中的30個位置(維度中為32的D*H*W), 並對原始在對應image_embedding空間上的特徵進行插值(僅限這30個點)\n",
      "        因此結果會是[1,256,1,1,30], 最後一個維度代表其中某一個通道在這30個點中的特徵值\n",
      "        \n",
      "\n",
      "        接下來squeeze去除1維度\n",
      "        \n",
      "point_embedding after squeeze torch.Size([1, 1, 30])\n",
      "point_pe after squeeze torch.Size([1, 256, 30])\n",
      "\n",
      "        permute後, 現在我們有包含了點座標資訊的point_embedding特徵以及包含了點座標資訊的point_pe(一個固定的位置編碼矩陣)\n",
      "        \n",
      "point_embedding after permute torch.Size([1, 30, 1])\n",
      "point_pe after permute torch.Size([1, 30, 256])\n",
      "\n",
      "        把沒有經過給定點插植特徵的原始資料也做flatten & permute\n",
      "        \n",
      "image_embedding after flatten & permute torch.Size([1, 1048576, 1])\n",
      "image_pe after flatten & permute torch.Size([1, 32768, 256])\n",
      "\n",
      "        image_embedding\t[1, 32768, 256]\n",
      "        image_pe    [1, 32768, 256]\n",
      "        point_embedding\t[1, 30, 256]\n",
      "        point_pe    [1, 30, 256]\n",
      "        全都丟進transformer block\n",
      "        \n",
      "\n",
      "                ======\n",
      "                call transformer layer 1\n",
      "                ======\n",
      "            \n",
      "global_query torch.Size([1, 10, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 256 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jacky\\Desktop\\3DSAM-adapter\\3DSAM-adapter\\modeling\\Med_SAM\\test copy.ipynb 儲存格 3\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jacky/Desktop/3DSAM-adapter/3DSAM-adapter/modeling/Med_SAM/test%20copy.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prompt_encoder\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jacky/Desktop/3DSAM-adapter/3DSAM-adapter/modeling/Med_SAM/test%20copy.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m patch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jacky/Desktop/3DSAM-adapter/3DSAM-adapter/modeling/Med_SAM/test%20copy.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ans \u001b[39m=\u001b[39m prompt_encoder(foo_feature, point_coord, [patch_size, patch_size, patch_size]) \u001b[39m# ?, [1,30,3], [128,128,128]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jacky/Desktop/3DSAM-adapter/3DSAM-adapter/modeling/Med_SAM/test%20copy.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ans\u001b[39m.\u001b[39msize() \u001b[39m# 1, 256, 32, 32, 32\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\anaconda3\\envs\\med_sam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\Desktop\\3DSAM-adapter\\3DSAM-adapter\\modeling\\Med_SAM\\prompt_encoder.py:446\u001b[0m, in \u001b[0;36mPromptEncoder.forward\u001b[1;34m(self, image_embeddings, point_coord, img_size, feat_size)\u001b[0m\n\u001b[0;32m    444\u001b[0m point_coord \u001b[39m=\u001b[39m point_coord\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m) \u001b[39m# 1,1,1,30,3\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m送進transformer的三個參數image_embeddings, image_pe, point_coord\u001b[39m\u001b[39m\"\u001b[39m, image_embeddings\u001b[39m.\u001b[39msize(), image_pe\u001b[39m.\u001b[39msize(), point_coord\u001b[39m.\u001b[39msize())\n\u001b[1;32m--> 446\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(image_embeddings, image_pe, point_coord)\n\u001b[0;32m    447\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape([\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m feat_size)\n\u001b[0;32m    449\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\anaconda3\\envs\\med_sam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\Desktop\\3DSAM-adapter\\3DSAM-adapter\\modeling\\Med_SAM\\prompt_encoder.py:186\u001b[0m, in \u001b[0;36mTwoWayTransformer.forward\u001b[1;34m(self, image_embedding, image_pe, point_coord)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers): \u001b[39m# 2個block\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[39m        ======\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[39m        call transformer layer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m        ======\u001b[39m\n\u001b[0;32m    185\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     image_embedding, point_embedding \u001b[39m=\u001b[39m layer(\n\u001b[0;32m    187\u001b[0m         image_embedding,\n\u001b[0;32m    188\u001b[0m         point_embedding,\n\u001b[0;32m    189\u001b[0m         image_pe,\n\u001b[0;32m    190\u001b[0m         point_pe,\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtransformer回傳\u001b[39m\u001b[39m\"\u001b[39m, image_embedding\u001b[39m.\u001b[39msize())\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m image_embedding\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\anaconda3\\envs\\med_sam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacky\\Desktop\\3DSAM-adapter\\3DSAM-adapter\\modeling\\Med_SAM\\prompt_encoder.py:253\u001b[0m, in \u001b[0;36mTwoWayAttentionBlock.forward\u001b[1;34m(self, img_embed, point_embed, img_pe, point_pe)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img_embed, point_embed, img_pe, point_pe) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m    246\u001b[0m     \u001b[39m# img_pe, point_pe沒用==???\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[39m# image_embedding\t[1, 32768, 256]\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[39m# image_pe    [1, 32768, 256] (沒用)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39m# point_embedding\t[1, 30, 256]\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[39m# point_pe    [1, 30, 256] (沒用)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mglobal_query\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_query\u001b[39m.\u001b[39msize())\n\u001b[1;32m--> 253\u001b[0m     q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_query, point_embed], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# [1,10,256]+[1,30,256]=[1,40,256]\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     self_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(q\u001b[39m=\u001b[39mq, k\u001b[39m=\u001b[39mq, v\u001b[39m=\u001b[39mq) \u001b[39m# 一開始都一樣[1,40,256], 包含了全局(自己學?)以及給定點的256個特徵，return [1, 40, 256]\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     self_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(self_out)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 256 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "foo_feature = torch.randn(1,1,256,64,64) # 1, 256, ?, ?, ?\n",
    "prompt_encoder = PromptEncoder(transformer=TwoWayTransformer(depth=2,\n",
    "                                                                 embedding_dim=256,\n",
    "                                                                 mlp_dim=2048,\n",
    "                                                                 num_heads=8))\n",
    "prompt_encoder.to(\"cpu\")\n",
    "patch_size=128\n",
    "ans = prompt_encoder(foo_feature, point_coord, [patch_size, patch_size, patch_size]) # ?, [1,30,3], [128,128,128]\n",
    "\n",
    "ans.size() # 1, 256, 32, 32, 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
