{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_embed',\n",
       " 'depth_embed',\n",
       " 'patch_embed.proj.weight',\n",
       " 'patch_embed.proj.bias',\n",
       " 'slice_embed.weight',\n",
       " 'slice_embed.bias',\n",
       " 'blocks.0.norm1.weight',\n",
       " 'blocks.0.norm1.bias',\n",
       " 'blocks.0.attn.rel_pos_h',\n",
       " 'blocks.0.attn.rel_pos_w',\n",
       " 'blocks.0.attn.rel_pos_d',\n",
       " 'blocks.0.attn.lr',\n",
       " 'blocks.0.attn.qkv.weight',\n",
       " 'blocks.0.attn.qkv.bias',\n",
       " 'blocks.0.attn.proj.weight',\n",
       " 'blocks.0.attn.proj.bias',\n",
       " 'blocks.0.norm2.weight',\n",
       " 'blocks.0.norm2.bias',\n",
       " 'blocks.0.mlp.lin1.weight',\n",
       " 'blocks.0.mlp.lin1.bias',\n",
       " 'blocks.0.mlp.lin2.weight',\n",
       " 'blocks.0.mlp.lin2.bias',\n",
       " 'blocks.0.adapter.linear1.weight',\n",
       " 'blocks.0.adapter.linear1.bias',\n",
       " 'blocks.0.adapter.conv.weight',\n",
       " 'blocks.0.adapter.conv.bias',\n",
       " 'blocks.0.adapter.linear2.weight',\n",
       " 'blocks.0.adapter.linear2.bias',\n",
       " 'blocks.1.norm1.weight',\n",
       " 'blocks.1.norm1.bias',\n",
       " 'blocks.1.attn.rel_pos_h',\n",
       " 'blocks.1.attn.rel_pos_w',\n",
       " 'blocks.1.attn.rel_pos_d',\n",
       " 'blocks.1.attn.lr',\n",
       " 'blocks.1.attn.qkv.weight',\n",
       " 'blocks.1.attn.qkv.bias',\n",
       " 'blocks.1.attn.proj.weight',\n",
       " 'blocks.1.attn.proj.bias',\n",
       " 'blocks.1.norm2.weight',\n",
       " 'blocks.1.norm2.bias',\n",
       " 'blocks.1.mlp.lin1.weight',\n",
       " 'blocks.1.mlp.lin1.bias',\n",
       " 'blocks.1.mlp.lin2.weight',\n",
       " 'blocks.1.mlp.lin2.bias',\n",
       " 'blocks.1.adapter.linear1.weight',\n",
       " 'blocks.1.adapter.linear1.bias',\n",
       " 'blocks.1.adapter.conv.weight',\n",
       " 'blocks.1.adapter.conv.bias',\n",
       " 'blocks.1.adapter.linear2.weight',\n",
       " 'blocks.1.adapter.linear2.bias',\n",
       " 'blocks.2.norm1.weight',\n",
       " 'blocks.2.norm1.bias',\n",
       " 'blocks.2.attn.rel_pos_h',\n",
       " 'blocks.2.attn.rel_pos_w',\n",
       " 'blocks.2.attn.rel_pos_d',\n",
       " 'blocks.2.attn.lr',\n",
       " 'blocks.2.attn.qkv.weight',\n",
       " 'blocks.2.attn.qkv.bias',\n",
       " 'blocks.2.attn.proj.weight',\n",
       " 'blocks.2.attn.proj.bias',\n",
       " 'blocks.2.norm2.weight',\n",
       " 'blocks.2.norm2.bias',\n",
       " 'blocks.2.mlp.lin1.weight',\n",
       " 'blocks.2.mlp.lin1.bias',\n",
       " 'blocks.2.mlp.lin2.weight',\n",
       " 'blocks.2.mlp.lin2.bias',\n",
       " 'blocks.2.adapter.linear1.weight',\n",
       " 'blocks.2.adapter.linear1.bias',\n",
       " 'blocks.2.adapter.conv.weight',\n",
       " 'blocks.2.adapter.conv.bias',\n",
       " 'blocks.2.adapter.linear2.weight',\n",
       " 'blocks.2.adapter.linear2.bias',\n",
       " 'blocks.3.norm1.weight',\n",
       " 'blocks.3.norm1.bias',\n",
       " 'blocks.3.attn.rel_pos_h',\n",
       " 'blocks.3.attn.rel_pos_w',\n",
       " 'blocks.3.attn.rel_pos_d',\n",
       " 'blocks.3.attn.lr',\n",
       " 'blocks.3.attn.qkv.weight',\n",
       " 'blocks.3.attn.qkv.bias',\n",
       " 'blocks.3.attn.proj.weight',\n",
       " 'blocks.3.attn.proj.bias',\n",
       " 'blocks.3.norm2.weight',\n",
       " 'blocks.3.norm2.bias',\n",
       " 'blocks.3.mlp.lin1.weight',\n",
       " 'blocks.3.mlp.lin1.bias',\n",
       " 'blocks.3.mlp.lin2.weight',\n",
       " 'blocks.3.mlp.lin2.bias',\n",
       " 'blocks.3.adapter.linear1.weight',\n",
       " 'blocks.3.adapter.linear1.bias',\n",
       " 'blocks.3.adapter.conv.weight',\n",
       " 'blocks.3.adapter.conv.bias',\n",
       " 'blocks.3.adapter.linear2.weight',\n",
       " 'blocks.3.adapter.linear2.bias',\n",
       " 'blocks.4.norm1.weight',\n",
       " 'blocks.4.norm1.bias',\n",
       " 'blocks.4.attn.rel_pos_h',\n",
       " 'blocks.4.attn.rel_pos_w',\n",
       " 'blocks.4.attn.rel_pos_d',\n",
       " 'blocks.4.attn.lr',\n",
       " 'blocks.4.attn.qkv.weight',\n",
       " 'blocks.4.attn.qkv.bias',\n",
       " 'blocks.4.attn.proj.weight',\n",
       " 'blocks.4.attn.proj.bias',\n",
       " 'blocks.4.norm2.weight',\n",
       " 'blocks.4.norm2.bias',\n",
       " 'blocks.4.mlp.lin1.weight',\n",
       " 'blocks.4.mlp.lin1.bias',\n",
       " 'blocks.4.mlp.lin2.weight',\n",
       " 'blocks.4.mlp.lin2.bias',\n",
       " 'blocks.4.adapter.linear1.weight',\n",
       " 'blocks.4.adapter.linear1.bias',\n",
       " 'blocks.4.adapter.conv.weight',\n",
       " 'blocks.4.adapter.conv.bias',\n",
       " 'blocks.4.adapter.linear2.weight',\n",
       " 'blocks.4.adapter.linear2.bias',\n",
       " 'blocks.5.norm1.weight',\n",
       " 'blocks.5.norm1.bias',\n",
       " 'blocks.5.attn.rel_pos_h',\n",
       " 'blocks.5.attn.rel_pos_w',\n",
       " 'blocks.5.attn.rel_pos_d',\n",
       " 'blocks.5.attn.lr',\n",
       " 'blocks.5.attn.qkv.weight',\n",
       " 'blocks.5.attn.qkv.bias',\n",
       " 'blocks.5.attn.proj.weight',\n",
       " 'blocks.5.attn.proj.bias',\n",
       " 'blocks.5.norm2.weight',\n",
       " 'blocks.5.norm2.bias',\n",
       " 'blocks.5.mlp.lin1.weight',\n",
       " 'blocks.5.mlp.lin1.bias',\n",
       " 'blocks.5.mlp.lin2.weight',\n",
       " 'blocks.5.mlp.lin2.bias',\n",
       " 'blocks.5.adapter.linear1.weight',\n",
       " 'blocks.5.adapter.linear1.bias',\n",
       " 'blocks.5.adapter.conv.weight',\n",
       " 'blocks.5.adapter.conv.bias',\n",
       " 'blocks.5.adapter.linear2.weight',\n",
       " 'blocks.5.adapter.linear2.bias',\n",
       " 'blocks.6.norm1.weight',\n",
       " 'blocks.6.norm1.bias',\n",
       " 'blocks.6.attn.rel_pos_h',\n",
       " 'blocks.6.attn.rel_pos_w',\n",
       " 'blocks.6.attn.rel_pos_d',\n",
       " 'blocks.6.attn.lr',\n",
       " 'blocks.6.attn.qkv.weight',\n",
       " 'blocks.6.attn.qkv.bias',\n",
       " 'blocks.6.attn.proj.weight',\n",
       " 'blocks.6.attn.proj.bias',\n",
       " 'blocks.6.norm2.weight',\n",
       " 'blocks.6.norm2.bias',\n",
       " 'blocks.6.mlp.lin1.weight',\n",
       " 'blocks.6.mlp.lin1.bias',\n",
       " 'blocks.6.mlp.lin2.weight',\n",
       " 'blocks.6.mlp.lin2.bias',\n",
       " 'blocks.6.adapter.linear1.weight',\n",
       " 'blocks.6.adapter.linear1.bias',\n",
       " 'blocks.6.adapter.conv.weight',\n",
       " 'blocks.6.adapter.conv.bias',\n",
       " 'blocks.6.adapter.linear2.weight',\n",
       " 'blocks.6.adapter.linear2.bias',\n",
       " 'blocks.7.norm1.weight',\n",
       " 'blocks.7.norm1.bias',\n",
       " 'blocks.7.attn.rel_pos_h',\n",
       " 'blocks.7.attn.rel_pos_w',\n",
       " 'blocks.7.attn.rel_pos_d',\n",
       " 'blocks.7.attn.lr',\n",
       " 'blocks.7.attn.qkv.weight',\n",
       " 'blocks.7.attn.qkv.bias',\n",
       " 'blocks.7.attn.proj.weight',\n",
       " 'blocks.7.attn.proj.bias',\n",
       " 'blocks.7.norm2.weight',\n",
       " 'blocks.7.norm2.bias',\n",
       " 'blocks.7.mlp.lin1.weight',\n",
       " 'blocks.7.mlp.lin1.bias',\n",
       " 'blocks.7.mlp.lin2.weight',\n",
       " 'blocks.7.mlp.lin2.bias',\n",
       " 'blocks.7.adapter.linear1.weight',\n",
       " 'blocks.7.adapter.linear1.bias',\n",
       " 'blocks.7.adapter.conv.weight',\n",
       " 'blocks.7.adapter.conv.bias',\n",
       " 'blocks.7.adapter.linear2.weight',\n",
       " 'blocks.7.adapter.linear2.bias',\n",
       " 'blocks.8.norm1.weight',\n",
       " 'blocks.8.norm1.bias',\n",
       " 'blocks.8.attn.rel_pos_h',\n",
       " 'blocks.8.attn.rel_pos_w',\n",
       " 'blocks.8.attn.rel_pos_d',\n",
       " 'blocks.8.attn.lr',\n",
       " 'blocks.8.attn.qkv.weight',\n",
       " 'blocks.8.attn.qkv.bias',\n",
       " 'blocks.8.attn.proj.weight',\n",
       " 'blocks.8.attn.proj.bias',\n",
       " 'blocks.8.norm2.weight',\n",
       " 'blocks.8.norm2.bias',\n",
       " 'blocks.8.mlp.lin1.weight',\n",
       " 'blocks.8.mlp.lin1.bias',\n",
       " 'blocks.8.mlp.lin2.weight',\n",
       " 'blocks.8.mlp.lin2.bias',\n",
       " 'blocks.8.adapter.linear1.weight',\n",
       " 'blocks.8.adapter.linear1.bias',\n",
       " 'blocks.8.adapter.conv.weight',\n",
       " 'blocks.8.adapter.conv.bias',\n",
       " 'blocks.8.adapter.linear2.weight',\n",
       " 'blocks.8.adapter.linear2.bias',\n",
       " 'blocks.9.norm1.weight',\n",
       " 'blocks.9.norm1.bias',\n",
       " 'blocks.9.attn.rel_pos_h',\n",
       " 'blocks.9.attn.rel_pos_w',\n",
       " 'blocks.9.attn.rel_pos_d',\n",
       " 'blocks.9.attn.lr',\n",
       " 'blocks.9.attn.qkv.weight',\n",
       " 'blocks.9.attn.qkv.bias',\n",
       " 'blocks.9.attn.proj.weight',\n",
       " 'blocks.9.attn.proj.bias',\n",
       " 'blocks.9.norm2.weight',\n",
       " 'blocks.9.norm2.bias',\n",
       " 'blocks.9.mlp.lin1.weight',\n",
       " 'blocks.9.mlp.lin1.bias',\n",
       " 'blocks.9.mlp.lin2.weight',\n",
       " 'blocks.9.mlp.lin2.bias',\n",
       " 'blocks.9.adapter.linear1.weight',\n",
       " 'blocks.9.adapter.linear1.bias',\n",
       " 'blocks.9.adapter.conv.weight',\n",
       " 'blocks.9.adapter.conv.bias',\n",
       " 'blocks.9.adapter.linear2.weight',\n",
       " 'blocks.9.adapter.linear2.bias',\n",
       " 'blocks.10.norm1.weight',\n",
       " 'blocks.10.norm1.bias',\n",
       " 'blocks.10.attn.rel_pos_h',\n",
       " 'blocks.10.attn.rel_pos_w',\n",
       " 'blocks.10.attn.rel_pos_d',\n",
       " 'blocks.10.attn.lr',\n",
       " 'blocks.10.attn.qkv.weight',\n",
       " 'blocks.10.attn.qkv.bias',\n",
       " 'blocks.10.attn.proj.weight',\n",
       " 'blocks.10.attn.proj.bias',\n",
       " 'blocks.10.norm2.weight',\n",
       " 'blocks.10.norm2.bias',\n",
       " 'blocks.10.mlp.lin1.weight',\n",
       " 'blocks.10.mlp.lin1.bias',\n",
       " 'blocks.10.mlp.lin2.weight',\n",
       " 'blocks.10.mlp.lin2.bias',\n",
       " 'blocks.10.adapter.linear1.weight',\n",
       " 'blocks.10.adapter.linear1.bias',\n",
       " 'blocks.10.adapter.conv.weight',\n",
       " 'blocks.10.adapter.conv.bias',\n",
       " 'blocks.10.adapter.linear2.weight',\n",
       " 'blocks.10.adapter.linear2.bias',\n",
       " 'blocks.11.norm1.weight',\n",
       " 'blocks.11.norm1.bias',\n",
       " 'blocks.11.attn.rel_pos_h',\n",
       " 'blocks.11.attn.rel_pos_w',\n",
       " 'blocks.11.attn.rel_pos_d',\n",
       " 'blocks.11.attn.lr',\n",
       " 'blocks.11.attn.qkv.weight',\n",
       " 'blocks.11.attn.qkv.bias',\n",
       " 'blocks.11.attn.proj.weight',\n",
       " 'blocks.11.attn.proj.bias',\n",
       " 'blocks.11.norm2.weight',\n",
       " 'blocks.11.norm2.bias',\n",
       " 'blocks.11.mlp.lin1.weight',\n",
       " 'blocks.11.mlp.lin1.bias',\n",
       " 'blocks.11.mlp.lin2.weight',\n",
       " 'blocks.11.mlp.lin2.bias',\n",
       " 'blocks.11.adapter.linear1.weight',\n",
       " 'blocks.11.adapter.linear1.bias',\n",
       " 'blocks.11.adapter.conv.weight',\n",
       " 'blocks.11.adapter.conv.bias',\n",
       " 'blocks.11.adapter.linear2.weight',\n",
       " 'blocks.11.adapter.linear2.bias',\n",
       " 'neck_3d.0.0.weight',\n",
       " 'neck_3d.0.1.weight',\n",
       " 'neck_3d.0.1.bias',\n",
       " 'neck_3d.0.2.weight',\n",
       " 'neck_3d.0.3.weight',\n",
       " 'neck_3d.0.3.bias',\n",
       " 'neck_3d.1.0.weight',\n",
       " 'neck_3d.1.1.weight',\n",
       " 'neck_3d.1.1.bias',\n",
       " 'neck_3d.1.2.weight',\n",
       " 'neck_3d.1.3.weight',\n",
       " 'neck_3d.1.3.bias',\n",
       " 'neck_3d.2.0.weight',\n",
       " 'neck_3d.2.1.weight',\n",
       " 'neck_3d.2.1.bias',\n",
       " 'neck_3d.2.2.weight',\n",
       " 'neck_3d.2.3.weight',\n",
       " 'neck_3d.2.3.bias',\n",
       " 'neck_3d.3.0.weight',\n",
       " 'neck_3d.3.1.weight',\n",
       " 'neck_3d.3.1.bias',\n",
       " 'neck_3d.3.2.weight',\n",
       " 'neck_3d.3.3.weight',\n",
       " 'neck_3d.3.3.bias']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from image_encoder import ImageEncoderViT_3d_v2 as ImageEncoderViT_3d\n",
    "from functools import partial\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"../../ckpt/sam_vit_b_01ec64.pth\")\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "img_encoder = ImageEncoderViT_3d(\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    window_size=14,\n",
    "    cubic_window_size=8,\n",
    "    out_chans=256,\n",
    "    num_slice = 16)\n",
    "img_encoder.load_state_dict(mask_generator.predictor.model.image_encoder.state_dict(), strict=False)\n",
    "del sam\n",
    "[i for i, _ in img_encoder.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_embed',\n",
       " 'patch_embed.proj.weight',\n",
       " 'patch_embed.proj.bias',\n",
       " 'blocks.0.norm1.weight',\n",
       " 'blocks.0.norm1.bias',\n",
       " 'blocks.0.attn.rel_pos_h',\n",
       " 'blocks.0.attn.rel_pos_w',\n",
       " 'blocks.0.attn.qkv.weight',\n",
       " 'blocks.0.attn.qkv.bias',\n",
       " 'blocks.0.attn.proj.weight',\n",
       " 'blocks.0.attn.proj.bias',\n",
       " 'blocks.0.norm2.weight',\n",
       " 'blocks.0.norm2.bias',\n",
       " 'blocks.0.mlp.lin1.weight',\n",
       " 'blocks.0.mlp.lin1.bias',\n",
       " 'blocks.0.mlp.lin2.weight',\n",
       " 'blocks.0.mlp.lin2.bias',\n",
       " 'blocks.1.norm1.weight',\n",
       " 'blocks.1.norm1.bias',\n",
       " 'blocks.1.attn.rel_pos_h',\n",
       " 'blocks.1.attn.rel_pos_w',\n",
       " 'blocks.1.attn.qkv.weight',\n",
       " 'blocks.1.attn.qkv.bias',\n",
       " 'blocks.1.attn.proj.weight',\n",
       " 'blocks.1.attn.proj.bias',\n",
       " 'blocks.1.norm2.weight',\n",
       " 'blocks.1.norm2.bias',\n",
       " 'blocks.1.mlp.lin1.weight',\n",
       " 'blocks.1.mlp.lin1.bias',\n",
       " 'blocks.1.mlp.lin2.weight',\n",
       " 'blocks.1.mlp.lin2.bias',\n",
       " 'blocks.2.norm1.weight',\n",
       " 'blocks.2.norm1.bias',\n",
       " 'blocks.2.attn.rel_pos_h',\n",
       " 'blocks.2.attn.rel_pos_w',\n",
       " 'blocks.2.attn.qkv.weight',\n",
       " 'blocks.2.attn.qkv.bias',\n",
       " 'blocks.2.attn.proj.weight',\n",
       " 'blocks.2.attn.proj.bias',\n",
       " 'blocks.2.norm2.weight',\n",
       " 'blocks.2.norm2.bias',\n",
       " 'blocks.2.mlp.lin1.weight',\n",
       " 'blocks.2.mlp.lin1.bias',\n",
       " 'blocks.2.mlp.lin2.weight',\n",
       " 'blocks.2.mlp.lin2.bias',\n",
       " 'blocks.3.norm1.weight',\n",
       " 'blocks.3.norm1.bias',\n",
       " 'blocks.3.attn.rel_pos_h',\n",
       " 'blocks.3.attn.rel_pos_w',\n",
       " 'blocks.3.attn.qkv.weight',\n",
       " 'blocks.3.attn.qkv.bias',\n",
       " 'blocks.3.attn.proj.weight',\n",
       " 'blocks.3.attn.proj.bias',\n",
       " 'blocks.3.norm2.weight',\n",
       " 'blocks.3.norm2.bias',\n",
       " 'blocks.3.mlp.lin1.weight',\n",
       " 'blocks.3.mlp.lin1.bias',\n",
       " 'blocks.3.mlp.lin2.weight',\n",
       " 'blocks.3.mlp.lin2.bias',\n",
       " 'blocks.4.norm1.weight',\n",
       " 'blocks.4.norm1.bias',\n",
       " 'blocks.4.attn.rel_pos_h',\n",
       " 'blocks.4.attn.rel_pos_w',\n",
       " 'blocks.4.attn.qkv.weight',\n",
       " 'blocks.4.attn.qkv.bias',\n",
       " 'blocks.4.attn.proj.weight',\n",
       " 'blocks.4.attn.proj.bias',\n",
       " 'blocks.4.norm2.weight',\n",
       " 'blocks.4.norm2.bias',\n",
       " 'blocks.4.mlp.lin1.weight',\n",
       " 'blocks.4.mlp.lin1.bias',\n",
       " 'blocks.4.mlp.lin2.weight',\n",
       " 'blocks.4.mlp.lin2.bias',\n",
       " 'blocks.5.norm1.weight',\n",
       " 'blocks.5.norm1.bias',\n",
       " 'blocks.5.attn.rel_pos_h',\n",
       " 'blocks.5.attn.rel_pos_w',\n",
       " 'blocks.5.attn.qkv.weight',\n",
       " 'blocks.5.attn.qkv.bias',\n",
       " 'blocks.5.attn.proj.weight',\n",
       " 'blocks.5.attn.proj.bias',\n",
       " 'blocks.5.norm2.weight',\n",
       " 'blocks.5.norm2.bias',\n",
       " 'blocks.5.mlp.lin1.weight',\n",
       " 'blocks.5.mlp.lin1.bias',\n",
       " 'blocks.5.mlp.lin2.weight',\n",
       " 'blocks.5.mlp.lin2.bias',\n",
       " 'blocks.6.norm1.weight',\n",
       " 'blocks.6.norm1.bias',\n",
       " 'blocks.6.attn.rel_pos_h',\n",
       " 'blocks.6.attn.rel_pos_w',\n",
       " 'blocks.6.attn.qkv.weight',\n",
       " 'blocks.6.attn.qkv.bias',\n",
       " 'blocks.6.attn.proj.weight',\n",
       " 'blocks.6.attn.proj.bias',\n",
       " 'blocks.6.norm2.weight',\n",
       " 'blocks.6.norm2.bias',\n",
       " 'blocks.6.mlp.lin1.weight',\n",
       " 'blocks.6.mlp.lin1.bias',\n",
       " 'blocks.6.mlp.lin2.weight',\n",
       " 'blocks.6.mlp.lin2.bias',\n",
       " 'blocks.7.norm1.weight',\n",
       " 'blocks.7.norm1.bias',\n",
       " 'blocks.7.attn.rel_pos_h',\n",
       " 'blocks.7.attn.rel_pos_w',\n",
       " 'blocks.7.attn.qkv.weight',\n",
       " 'blocks.7.attn.qkv.bias',\n",
       " 'blocks.7.attn.proj.weight',\n",
       " 'blocks.7.attn.proj.bias',\n",
       " 'blocks.7.norm2.weight',\n",
       " 'blocks.7.norm2.bias',\n",
       " 'blocks.7.mlp.lin1.weight',\n",
       " 'blocks.7.mlp.lin1.bias',\n",
       " 'blocks.7.mlp.lin2.weight',\n",
       " 'blocks.7.mlp.lin2.bias',\n",
       " 'blocks.8.norm1.weight',\n",
       " 'blocks.8.norm1.bias',\n",
       " 'blocks.8.attn.rel_pos_h',\n",
       " 'blocks.8.attn.rel_pos_w',\n",
       " 'blocks.8.attn.qkv.weight',\n",
       " 'blocks.8.attn.qkv.bias',\n",
       " 'blocks.8.attn.proj.weight',\n",
       " 'blocks.8.attn.proj.bias',\n",
       " 'blocks.8.norm2.weight',\n",
       " 'blocks.8.norm2.bias',\n",
       " 'blocks.8.mlp.lin1.weight',\n",
       " 'blocks.8.mlp.lin1.bias',\n",
       " 'blocks.8.mlp.lin2.weight',\n",
       " 'blocks.8.mlp.lin2.bias',\n",
       " 'blocks.9.norm1.weight',\n",
       " 'blocks.9.norm1.bias',\n",
       " 'blocks.9.attn.rel_pos_h',\n",
       " 'blocks.9.attn.rel_pos_w',\n",
       " 'blocks.9.attn.qkv.weight',\n",
       " 'blocks.9.attn.qkv.bias',\n",
       " 'blocks.9.attn.proj.weight',\n",
       " 'blocks.9.attn.proj.bias',\n",
       " 'blocks.9.norm2.weight',\n",
       " 'blocks.9.norm2.bias',\n",
       " 'blocks.9.mlp.lin1.weight',\n",
       " 'blocks.9.mlp.lin1.bias',\n",
       " 'blocks.9.mlp.lin2.weight',\n",
       " 'blocks.9.mlp.lin2.bias',\n",
       " 'blocks.10.norm1.weight',\n",
       " 'blocks.10.norm1.bias',\n",
       " 'blocks.10.attn.rel_pos_h',\n",
       " 'blocks.10.attn.rel_pos_w',\n",
       " 'blocks.10.attn.qkv.weight',\n",
       " 'blocks.10.attn.qkv.bias',\n",
       " 'blocks.10.attn.proj.weight',\n",
       " 'blocks.10.attn.proj.bias',\n",
       " 'blocks.10.norm2.weight',\n",
       " 'blocks.10.norm2.bias',\n",
       " 'blocks.10.mlp.lin1.weight',\n",
       " 'blocks.10.mlp.lin1.bias',\n",
       " 'blocks.10.mlp.lin2.weight',\n",
       " 'blocks.10.mlp.lin2.bias',\n",
       " 'blocks.11.norm1.weight',\n",
       " 'blocks.11.norm1.bias',\n",
       " 'blocks.11.attn.rel_pos_h',\n",
       " 'blocks.11.attn.rel_pos_w',\n",
       " 'blocks.11.attn.qkv.weight',\n",
       " 'blocks.11.attn.qkv.bias',\n",
       " 'blocks.11.attn.proj.weight',\n",
       " 'blocks.11.attn.proj.bias',\n",
       " 'blocks.11.norm2.weight',\n",
       " 'blocks.11.norm2.bias',\n",
       " 'blocks.11.mlp.lin1.weight',\n",
       " 'blocks.11.mlp.lin1.bias',\n",
       " 'blocks.11.mlp.lin2.weight',\n",
       " 'blocks.11.mlp.lin2.bias',\n",
       " 'neck.0.weight',\n",
       " 'neck.1.weight',\n",
       " 'neck.1.bias',\n",
       " 'neck.2.weight',\n",
       " 'neck.3.weight',\n",
       " 'neck.3.bias']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in mask_generator.predictor.model.image_encoder.state_dict().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "H, W, D = 32, 32, 32  # 輸入張量的高度、寬度和深度\n",
    "img_mask = torch.zeros((1, H, W, D, 1))  # 創建一個形狀為[1, H, W, D, 1]的零張量\n",
    "\n",
    "# 定義滑動窗口的移位範圍\n",
    "h_slices = (slice(0, -8),\n",
    "            slice(-8, -4),\n",
    "            slice(-4, None))\n",
    "w_slices = (slice(0, -8),\n",
    "            slice(-8, -4),\n",
    "            slice(-4, None))\n",
    "d_slices = (slice(0, -8),\n",
    "            slice(-8, -4),\n",
    "            slice(-4, None))\n",
    "\n",
    "cnt = 0\n",
    "# 對每個窗口進行標籤\n",
    "for h in h_slices:\n",
    "    for w in w_slices:\n",
    "        for d in d_slices:\n",
    "            img_mask[:, h, w, d, :] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "print(img_mask[0,0:24,0:24,24:28,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2097152\n",
      "[ 970475 1634943  237443  278763  792592  183698 1383205 1848259 1226765\n",
      "  703493]\n",
      "tensor([[ 29],\n",
      "        [100],\n",
      "        [ 63],\n",
      "        [  1],\n",
      "        [ 48],\n",
      "        [ 27],\n",
      "        [ 54],\n",
      "        [103],\n",
      "        [112],\n",
      "        [120]])\n",
      "torch.Size([1, 30, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seg = torch.ones(1,128,128,128)\n",
    "l = len(torch.where(seg == 1)[0])\n",
    "print(l)\n",
    "sample = np.random.choice(np.arange(l), 10, replace=True) # 從範圍為 [0, l) 的整數中隨機選取 10 個數字（可能有重複）\n",
    "print(sample)\n",
    "x = torch.where(seg == 1)[1][sample].unsqueeze(1)\n",
    "y = torch.where(seg == 1)[3][sample].unsqueeze(1)\n",
    "z = torch.where(seg == 1)[2][sample].unsqueeze(1)\n",
    "print(z)\n",
    "point_coord = torch.cat([x, y, z], dim=1).unsqueeze(1).float() \n",
    "\n",
    "foo = torch.randn(1,20,3)\n",
    "point_coord = point_coord.transpose(0,1)\n",
    "point_coord = torch.cat([point_coord,foo],dim=1)\n",
    "print(point_coord.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "送進transformer的三個參數image_embeddings, image_pe, point_coord torch.Size([1, 256, 32, 32, 32]) torch.Size([1, 256, 32, 32, 32]) torch.Size([1, 1, 1, 30, 3])\n",
      "===init===\n",
      "image_embedding init torch.Size([1, 256, 32, 32, 32])\n",
      "point_coord init torch.Size([1, 1, 1, 30, 3])\n",
      "\n",
      "point_embedding after grid sample torch.Size([1, 256, 1, 1, 30])\n",
      "point_pe after grid sample torch.Size([1, 256, 1, 1, 30])\n",
      "\n",
      "        之所以維度由[1,256,32,32,32]變成[1,256,1,1,30], 是因為point_coord [1,1,1,30,3]中包含了30個xyz的座標(已正規化到-1~1之間)\n",
      "        定位了在image_embedding中的30個位置(維度中為32的D*H*W), 並對原始在對應image_embedding空間上的特徵進行插值(僅限這30個點)\n",
      "        因此結果會是[1,256,1,1,30], 最後一個維度代表其中某一個通道在這30個點中的特徵值\n",
      "        \n",
      "\n",
      "        接下來squeeze去除1維度\n",
      "        \n",
      "point_embedding after squeeze torch.Size([1, 256, 30])\n",
      "point_pe after squeeze torch.Size([1, 256, 30])\n",
      "\n",
      "        permute後, 現在我們有包含了點座標資訊的point_embedding特徵以及包含了點座標資訊的point_pe(一個固定的位置編碼矩陣)\n",
      "        \n",
      "point_embedding after permute torch.Size([1, 30, 256])\n",
      "point_pe after permute torch.Size([1, 30, 256])\n",
      "\n",
      "        把沒有經過給定點插植特徵的原始資料也做flatten & permute\n",
      "        \n",
      "image_embedding after flatten & permute torch.Size([1, 32768, 256])\n",
      "image_pe after flatten & permute torch.Size([1, 32768, 256])\n",
      "\n",
      "        image_embedding\t[1, 32768, 256]\n",
      "        image_pe    [1, 32768, 256]\n",
      "        point_embedding\t[1, 30, 256]\n",
      "        point_pe    [1, 30, 256]\n",
      "        全都丟進transformer block\n",
      "        \n",
      "\n",
      "                ======\n",
      "                call transformer layer 1\n",
      "                ======\n",
      "            \n",
      "global_query torch.Size([1, 10, 256])\n",
      "QKV after 全連結層256 to 256 torch.Size([1, 40, 256])\n",
      "QKV after _separate_heads torch.Size([1, 8, 40, 32])\n",
      "QK做完矩陣運算 torch.Size([1, 8, 40, 40])\n",
      "out torch.Size([1, 40, 256])\n",
      "\n",
      "QKV after 全連結層256 to 256 torch.Size([1, 32768, 128])\n",
      "QKV after _separate_heads torch.Size([1, 8, 32768, 16])\n",
      "QK做完矩陣運算 torch.Size([1, 8, 32768, 10])\n",
      "out torch.Size([1, 32768, 256])\n",
      "\n",
      "\n",
      "                ======\n",
      "                call transformer layer 2\n",
      "                ======\n",
      "            \n",
      "global_query torch.Size([1, 10, 256])\n",
      "QKV after 全連結層256 to 256 torch.Size([1, 40, 256])\n",
      "QKV after _separate_heads torch.Size([1, 8, 40, 32])\n",
      "QK做完矩陣運算 torch.Size([1, 8, 40, 40])\n",
      "out torch.Size([1, 40, 256])\n",
      "\n",
      "QKV after 全連結層256 to 256 torch.Size([1, 32768, 128])\n",
      "QKV after _separate_heads torch.Size([1, 8, 32768, 16])\n",
      "QK做完矩陣運算 torch.Size([1, 8, 32768, 10])\n",
      "out torch.Size([1, 32768, 256])\n",
      "\n",
      "transformer回傳 torch.Size([1, 32768, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 32, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_feature = torch.randn(1,256,32,32,32) # 1, 256, ?, ?, ?\n",
    "prompt_encoder = PromptEncoder(transformer=TwoWayTransformer(depth=2,\n",
    "                                                                 embedding_dim=256,\n",
    "                                                                 mlp_dim=2048,\n",
    "                                                                 num_heads=8))\n",
    "prompt_encoder.to(\"cpu\")\n",
    "patch_size=128\n",
    "ans = prompt_encoder(foo_feature, point_coord, [patch_size, patch_size, patch_size]) # ?, [1,30,3], [128,128,128]\n",
    "\n",
    "ans.size() # 1, 256, 32, 32, 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
