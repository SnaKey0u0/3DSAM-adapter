{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"..\\\\..\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from modeling.Med_SAM.image_encoder import ImageEncoderViT_3d_v2 as ImageEncoderViT_3d\n",
    "from modeling.Med_SAM.prompt_encoder import PromptEncoder, TwoWayTransformer\n",
    "from modeling.Med_SAM.mask_decoder import VIT_MLAHead_h as VIT_MLAHead\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset.my_dataset import MyDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from monai.losses import DiceCELoss, DiceLoss\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import surface_distance\n",
    "from surface_distance import metrics\n",
    "\n",
    "\n",
    "num_prompts = 1\n",
    "patch_size = 128\n",
    "snapshot_path = \"C:\\\\Users\\\\Jacky\\\\Desktop\\\\3DSAM-adapter\\\\3DSAM-adapter\\\\snapshot\\\\show_model_result\\\\best.pth.tar\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dice_loss = DiceLoss(\n",
    "    include_background=False, softmax=False, to_onehot_y=True, reduction=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIT_MLAHead_h(\n",
       "  (mlahead): MLAHead(\n",
       "    (head2): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (head3): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (head4): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (head5): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (cls): Sequential(\n",
       "    (0): Conv3d(513, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(128, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image encoder\n",
    "img_encoder = ImageEncoderViT_3d(\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    window_size=14,\n",
    "    cubic_window_size=8,\n",
    "    out_chans=256,\n",
    "    num_slice=16,\n",
    ")\n",
    "\n",
    "img_encoder.load_state_dict(\n",
    "    torch.load(snapshot_path, map_location=\"cpu\")[\"encoder_dict\"],\n",
    "    strict=True,\n",
    ")\n",
    "img_encoder.to(device)\n",
    "\n",
    "# load prompt encoder\n",
    "prompt_encoder_list = []\n",
    "for i in range(4):\n",
    "    prompt_encoder = PromptEncoder(\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2, embedding_dim=256, mlp_dim=2048, num_heads=8\n",
    "        )\n",
    "    )\n",
    "    prompt_encoder.load_state_dict(\n",
    "        torch.load(snapshot_path, map_location=\"cpu\")[\"feature_dict\"][i],\n",
    "        strict=True,\n",
    "    )\n",
    "    prompt_encoder.to(device)\n",
    "    prompt_encoder_list.append(prompt_encoder)\n",
    "\n",
    "# load mask decoder\n",
    "mask_decoder = VIT_MLAHead(img_size=96).to(device)\n",
    "mask_decoder.load_state_dict(\n",
    "    torch.load(snapshot_path, map_location=\"cpu\")[\"decoder_dict\"],\n",
    "    strict=True,\n",
    ")\n",
    "mask_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 577\n",
      "val: 80\n",
      "test: 168\n"
     ]
    }
   ],
   "source": [
    "train_data = MyDataset(\"D:\\\\ds\", \"train\")\n",
    "val_data = MyDataset(\"D:\\\\ds\", \"val\")\n",
    "test_data = MyDataset(\"D:\\\\ds\", \"test\")\n",
    "print(\"train:\", len(train_data))\n",
    "print(\"val:\", len(val_data))\n",
    "print(\"test:\", len(test_data))\n",
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "val_data = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "test = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:\\\\Users\\\\Jacky\\\\Desktop\\\\3DSAM-adapter\\\\3DSAM-adapter\\\\snapshot\\\\show_model_result\\\\images\\\\{}\\\\slice_{}.png\"\n",
    "\n",
    "def plot_slices(img, predict, ground_truth, fname):\n",
    "    # 把img(有spacing和repeat)內插回來\n",
    "    img = F.interpolate(img, size=ground_truth.shape[2:], mode=\"trilinear\")\n",
    "    img = img[0, 0]\n",
    "    predict = predict.squeeze()\n",
    "    ground_truth = ground_truth.squeeze()\n",
    "\n",
    "    assert (\n",
    "        predict.shape == ground_truth.shape == img.shape\n",
    "    ), \"Shapes of predict, ground_truth and img must be the same.\"\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "        dice = 1 - dice_loss(predict[i][None, None], ground_truth[i][None, None])\n",
    "        dice = dice.squeeze().cpu().numpy()\n",
    "        fig, axes = plt.subplots(1, 3)\n",
    "        plt.suptitle(f\"DICE: {dice}\")\n",
    "        \n",
    "        # 將張量轉換為NumPy數組\n",
    "        img_np = img[i].cpu().numpy()\n",
    "        predict_np = predict[i].cpu().numpy()\n",
    "        ground_truth_np = ground_truth[i].cpu().numpy()\n",
    "\n",
    "        for j, (data, title) in enumerate(zip([img_np, predict_np, ground_truth_np], [\"Image\", \"Predict\", \"Ground Truth\"])):\n",
    "            axes[j].imshow(data, cmap=\"gray\")\n",
    "            axes[j].set_title(title)\n",
    "\n",
    "        # 儲存圖片\n",
    "        plt.savefig(save_path.format(fname, i))\n",
    "        \n",
    "        # 清除當前圖片以節省記憶體\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(img, prompt, img_encoder, prompt_encoder, mask_decoder):\n",
    "    out = F.interpolate(img.float(), scale_factor=256 / patch_size, mode=\"trilinear\")\n",
    "    input_batch = out[0].transpose(0, 1)\n",
    "    batch_features, feature_list = img_encoder(input_batch)\n",
    "    feature_list.append(batch_features)\n",
    "    # feature_list = feature_list[::-1]\n",
    "    points_torch = prompt.transpose(0, 1)\n",
    "    new_feature = []\n",
    "    for i, (feature, feature_decoder) in enumerate(zip(feature_list, prompt_encoder)):\n",
    "        if i == 3:\n",
    "            new_feature.append(\n",
    "                feature_decoder(\n",
    "                    feature.to(device),\n",
    "                    points_torch.clone(),\n",
    "                    [patch_size, patch_size, patch_size],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            new_feature.append(feature.to(device))\n",
    "    # new_feature => 4個[1,256,32,32,32]tensor的list\n",
    "    img_resize = F.interpolate( # torch.Size([1, 3, 128, 128, 128])=>torch.Size([1, 1, 64, 64, 64])\n",
    "        img[0, 0].permute(1, 2, 0).unsqueeze(0).unsqueeze(0).to(device),\n",
    "        scale_factor = 32 / patch_size,\n",
    "        mode=\"trilinear\",\n",
    "    )\n",
    "    new_feature.append(img_resize) # 除了4層feature，也加入內插過的原圖\n",
    "    masks = mask_decoder(new_feature, 2, patch_size // 32)\n",
    "    masks = masks.permute(0, 1, 4, 2, 3)\n",
    "    return masks #1,2,128,128,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\Jacky\\anaconda3\\envs\\med_sam\\lib\\site-packages\\monai\\losses\\dice.py:144: UserWarning: single channel prediction, `to_onehot_y=True` ignored.\n",
      "  warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n",
      "c:\\Users\\Jacky\\anaconda3\\envs\\med_sam\\lib\\site-packages\\monai\\losses\\dice.py:150: UserWarning: single channel prediction, `include_background=False` ignored.\n",
      "  warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n",
      "26it [01:25,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss_summary = []\n",
    "    class_name = None\n",
    "    last_name = None\n",
    "    index = {}\n",
    "    best = 0\n",
    "    worst = 1\n",
    "    for idx, (img, seg, name) in tqdm(enumerate(test_data)):\n",
    "        seg = seg.float().unsqueeze(0)\n",
    "        img = img.unsqueeze(0)\n",
    "        # 把seg (無spacing)內插成img(有spacing)的大小\n",
    "        prompt = F.interpolate(seg[None, :, :, :, :], img.shape[2:], mode=\"nearest\")[0]\n",
    "        seg = seg.to(device).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        seg_pred = torch.zeros_like(prompt).to(device)\n",
    "\n",
    "        # 隨機選num_prompts個正樣本的x, y, z座標為中心，選一個patch_size大小的立方體\n",
    "        l = len(torch.where(prompt == 1)[0])\n",
    "        sample = np.random.choice(np.arange(l), num_prompts, replace=True)\n",
    "\n",
    "        x = torch.where(prompt == 1)[1][sample].unsqueeze(1)\n",
    "        y = torch.where(prompt == 1)[3][sample].unsqueeze(1)\n",
    "        z = torch.where(prompt == 1)[2][sample].unsqueeze(1)\n",
    "\n",
    "        # x_m = (torch.max(x) + torch.min(x)) // 2\n",
    "        # y_m = (torch.max(y) + torch.min(y)) // 2\n",
    "        # z_m = (torch.max(z) + torch.min(z)) // 2\n",
    "        x_m = torch.div(torch.max(x) + torch.min(x), 2, rounding_mode=\"trunc\")\n",
    "        y_m = torch.div(torch.max(y) + torch.min(y), 2, rounding_mode=\"trunc\")\n",
    "        z_m = torch.div(torch.max(z) + torch.min(z), 2, rounding_mode=\"trunc\")\n",
    "\n",
    "        d_min = x_m - patch_size // 2\n",
    "        d_max = x_m + patch_size // 2\n",
    "        h_min = z_m - patch_size // 2\n",
    "        h_max = z_m + patch_size // 2\n",
    "        w_min = y_m - patch_size // 2\n",
    "        w_max = y_m + patch_size // 2\n",
    "        d_l = max(0, -d_min)\n",
    "        d_r = max(0, d_max - prompt.shape[1])\n",
    "        h_l = max(0, -h_min)\n",
    "        h_r = max(0, h_max - prompt.shape[2])\n",
    "        w_l = max(0, -w_min)\n",
    "        w_r = max(0, w_max - prompt.shape[3])\n",
    "\n",
    "        # 轉成相對位置座標(大概)\n",
    "        points = (\n",
    "            torch.cat([x - d_min, y - w_min, z - h_min], dim=1).unsqueeze(1).float()\n",
    "        )\n",
    "        points_torch = points.to(device)\n",
    "        d_min = max(0, d_min)\n",
    "        h_min = max(0, h_min)\n",
    "        w_min = max(0, w_min)\n",
    "        img_patch = img[:, :, d_min:d_max, h_min:h_max, w_min:w_max].clone()\n",
    "        img_patch = F.pad(img_patch, (w_l, w_r, h_l, h_r, d_l, d_r))\n",
    "        pred = model_predict(\n",
    "            img_patch, points_torch, img_encoder, prompt_encoder_list, mask_decoder\n",
    "        )\n",
    "        pred = pred[\n",
    "            :, :, d_l : patch_size - d_r, h_l : patch_size - h_r, w_l : patch_size - w_r\n",
    "        ]\n",
    "        pred = F.softmax(pred, dim=1)[:, 1]\n",
    "        seg_pred[:, d_min:d_max, h_min:h_max, w_min:w_max] += pred\n",
    "\n",
    "        # 把有spacing的結果內插回原始大小\n",
    "        final_pred = F.interpolate(\n",
    "            seg_pred.unsqueeze(1), size=seg.shape[2:], mode=\"trilinear\"\n",
    "        )\n",
    "        masks = final_pred > 0.5\n",
    "        loss = 1 - dice_loss(masks, seg)\n",
    "        loss = loss.squeeze().item()\n",
    "        loss_summary.append(loss)\n",
    "        # dictionary記住各部位最好和最差的volumn\n",
    "        if name != last_name:\n",
    "            if last_name is not None:\n",
    "                losses = np.array(loss_summary)\n",
    "                with open(\"loss_summary.txt\", \"a+\") as f:\n",
    "                    f.write(f\"{last_name}, max: {np.max(losses)}, min: {np.min(losses)}, median: {np.median(losses)}\\n\")\n",
    "                loss_summary = []\n",
    "                best = 0\n",
    "                worst = 1\n",
    "            last_name = name\n",
    "        if loss > best:\n",
    "            best = loss\n",
    "            if not os.path.isdir(f\"images\\\\{name}_best_loss\"):\n",
    "                os.makedirs(f\"images\\\\{name}_best_loss\")\n",
    "            index[f\"{name}_best_loss\"]=(img_patch, masks, seg)\n",
    "        if loss < worst:\n",
    "            worst = loss\n",
    "            if not os.path.isdir(f\"images\\\\{name}_worst_loss\"):\n",
    "                os.makedirs(f\"images\\\\{name}_worst_loss\")\n",
    "            index[f\"{name}_worst_loss\"]=(img_patch, masks, seg)\n",
    "losses = np.array(loss_summary)\n",
    "with open(\"loss_summary.txt\", \"a+\") as f:\n",
    "    f.write(f\"{last_name}, max: {np.max(losses)}, min: {np.min(losses)}, median: {np.median(losses)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存slice\n",
    "for k, (img_patch, masks, seg) in index.items():\n",
    "    plot_slices(img_patch, masks, seg, k)\n",
    "    print(k, \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for mode, data in enumerate([train_data, val_data]):\n",
    "#         for idx, (img, seg, name) in enumerate(data):\n",
    "#             print('seg: ', seg.sum())\n",
    "#             out = F.interpolate(img.float(), scale_factor=256 / patch_size, mode='trilinear')\n",
    "#             input_batch = out.to(device)\n",
    "#             input_batch = input_batch[0].transpose(0, 1)\n",
    "#             batch_features, feature_list = img_encoder(input_batch)\n",
    "#             feature_list.append(batch_features)\n",
    "#             #feature_list = feature_list[::-1]\n",
    "#             l = len(torch.where(seg == 1)[0])\n",
    "#             points_torch = None\n",
    "#             if l > 0:\n",
    "#                 sample = np.random.choice(np.arange(l), 10, replace=True)\n",
    "#                 x = torch.where(seg == 1)[1][sample].unsqueeze(1)\n",
    "#                 y = torch.where(seg == 1)[3][sample].unsqueeze(1)\n",
    "#                 z = torch.where(seg == 1)[2][sample].unsqueeze(1)\n",
    "#                 points = torch.cat([x, y, z], dim=1).unsqueeze(1).float()\n",
    "#                 points_torch = points.to(device)\n",
    "#                 points_torch = points_torch.transpose(0, 1)\n",
    "#             l = len(torch.where(seg < 10)[0])\n",
    "#             sample = np.random.choice(np.arange(l), 10, replace=True)\n",
    "#             x = torch.where(seg < 10)[1][sample].unsqueeze(1)\n",
    "#             y = torch.where(seg < 10)[3][sample].unsqueeze(1)\n",
    "#             z = torch.where(seg < 10)[2][sample].unsqueeze(1)\n",
    "#             points = torch.cat([x, y, z], dim=1).unsqueeze(1).float()\n",
    "#             points_torch_negative = points.to(device)\n",
    "#             points_torch_negative = points_torch_negative.transpose(0, 1)\n",
    "#             if points_torch is not None:\n",
    "#                 points_torch = points_torch\n",
    "#             else:\n",
    "#                 points_torch = points_torch_negative\n",
    "#             new_feature = []\n",
    "#             for i, (feature, prompt_encoder) in enumerate(zip(feature_list, prompt_encoder_list)):\n",
    "#                 if i == 3:\n",
    "#                     new_feature.append(\n",
    "#                         prompt_encoder(feature, points_torch.clone(), [patch_size, patch_size, patch_size])\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     new_feature.append(feature)\n",
    "#             img_resize = F.interpolate(img[:, 0].permute(0, 2, 3, 1).unsqueeze(1).to(device), scale_factor=32/patch_size,\n",
    "#                                         mode='trilinear')\n",
    "#             new_feature.append(img_resize)\n",
    "#             masks = mask_decoder(new_feature, 2, patch_size//32)\n",
    "#             masks = masks.permute(0, 1, 4, 2, 3)\n",
    "#             seg = seg.to(device)\n",
    "#             seg = seg.unsqueeze(1)\n",
    "#             loss = dice_loss(masks, seg)\n",
    "            \n",
    "#             if mode == 0:\n",
    "#                 if not os.path.isdir(f\"images\\\\{name[0]}_{idx}\"):\n",
    "#                     os.makedirs(f\"images\\\\train\\\\{name[0]}_{idx}\")\n",
    "#                 plot_slices(img_patch, masks, seg, f\"{name[0]}_{idx}\", \"train\")\n",
    "#             else:\n",
    "#                 if not os.path.isdir(f\"images\\\\{name[0]}_{idx}\"):\n",
    "#                     os.makedirs(f\"images\\\\val\\\\{name[0]}_{idx}\")\n",
    "#                 plot_slices(img_patch, masks, seg, f\"{name[0]}_{idx}\", \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
